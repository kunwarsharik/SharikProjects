{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reinforcement Q_Learning without library.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqKWazx7SksaZSzayUn5L8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunwarsharik/SharikProjects/blob/master/Reinforcement_Q_Learning_without_library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ce53qbasJQNU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "fcd56d6f-bc20-4052-85c0-6a76e6eef31d"
      },
      "source": [
        "!pip install cmake 'gym[atari]' scipy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.6/dist-packages (3.12.0)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.6/dist-packages (0.15.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.4.10)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.17.5)\n",
            "Requirement already satisfied: cloudpickle~=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (1.12.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (6.2.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (0.2.6)\n",
            "Requirement already satisfied: opencv-python; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjO1VJEaJfp7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym\n",
        "env = gym.make(\"Taxi-v3\").env"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8myPbkuJktP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "cf4014db-7247-4cb6-d135-e619919d92c8"
      },
      "source": [
        "env.render()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | :\u001b[43m \u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqA84c6bJr-E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af98aed3-4eb2-4848-8856-56ae09a610fe"
      },
      "source": [
        "#reset,step(action),render are useful methods of env interface\n",
        "#step returns obervation(of env),reward(action result benifit or penalty),done(completed the objective or not),info(performance,latency related information)\n",
        "env.reset()#return random state"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "491"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmQxDlnlQqFz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "e1d16d58-d2a8-406e-f405-ac082a5b54f1"
      },
      "source": [
        "env.render()\n",
        "env.action_space,env.observation_space\n",
        "#represent number of action possible and number of states in universe\n",
        "#0,1,2,3,4,5==south,north,east,west,pickup,dropoff also each state has its own particular number(id)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m:\u001b[43m \u001b[0m|\n",
            "+---------+\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Discrete(6), Discrete(500))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU9es5aERBTj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "7894967b-3947-4a62-96e4-3321f39a2c96"
      },
      "source": [
        "#by exploration RL algo will learn mapping state to optimal action\n",
        "state = env.encode(3,1,2,0)\n",
        "env.s=state#set the env row col pass, drop\n",
        "env.render()\n",
        "print(state)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "328\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GTCOBYsTHAZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2ec997c0-949e-49c8-d5e3-19ccac22f6c9"
      },
      "source": [
        "#with env an initial reward table of shape states x actions is also created\n",
        "# it is denoted by P\n",
        "env.P[328]#action probability, newstate, reward, done"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yWXhwOIUbsB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0284c7db-34fc-4c53-acbd-1e1091c5adc3"
      },
      "source": [
        "#try without RL\n",
        "epoch,penalties=0,0\n",
        "while True:\n",
        "    action=env.action_space.sample()\n",
        "    state,reward,done,info=env.step(action)\n",
        "    if reward == -10:\n",
        "        penalties +=1\n",
        "    epoch+=1\n",
        "    if done:\n",
        "        break\n",
        "print(epoch, penalties)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2169 716\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCgS09uQY1_q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#255 time-steps since we are not learning from prevoius experience\n",
        "#why not use rewards from each step to learn \n",
        "#this is q learning\n",
        "#q table stores value for (state,action) called q-value higher q value indicate good move"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmBTKwfruL7-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#q = q*(1-ap)+ap(reward+dis(max(q'))),ap is alpha,dis is gamma"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhSsUjj81iOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#there is a trade of b/w exploration and exploitation. to prevent agent to take same route always\n",
        "#we use epsilon, "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTfNdHTH1h9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wtUBLDtPHp-N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f9c522e5-d1e7-4469-f3d2-a536c3807170"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBYNhb65HXU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_table=np.zeros((env.observation_space.n,env.action_space.n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngn4HGm4RsGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x0kcGgdH8PR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3befe891-5342-42a2-c6c1-0f65c2d78e9a"
      },
      "source": [
        "alpha,gamma,epsilon=0.1,0.6,0.1\n",
        "for i in range(1, 100001):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward, = 0, 0, 0\n",
        "    while True:\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action=env.action_space.sample()\n",
        "        else:\n",
        "            action=np.argmax(q_table[state])\n",
        "        next_state,reward,done,info=env.step(action)\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "        if reward == -10:\n",
        "           penalties += 1\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "        if done:\n",
        "            break\n",
        "    if i %500==0:\n",
        "        print(penalties,epochs)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 119\n",
            "1 55\n",
            "2 38\n",
            "1 17\n",
            "0 12\n",
            "0 10\n",
            "0 9\n",
            "0 9\n",
            "1 19\n",
            "0 25\n",
            "0 14\n",
            "0 13\n",
            "0 17\n",
            "0 17\n",
            "0 13\n",
            "0 11\n",
            "1 13\n",
            "0 24\n",
            "0 21\n",
            "0 17\n",
            "0 15\n",
            "1 16\n",
            "0 15\n",
            "2 16\n",
            "3 20\n",
            "2 17\n",
            "0 10\n",
            "1 18\n",
            "1 12\n",
            "0 14\n",
            "0 11\n",
            "1 20\n",
            "0 22\n",
            "2 15\n",
            "0 10\n",
            "1 17\n",
            "0 19\n",
            "0 20\n",
            "0 19\n",
            "0 19\n",
            "0 14\n",
            "2 18\n",
            "0 13\n",
            "0 9\n",
            "0 17\n",
            "0 15\n",
            "0 11\n",
            "0 17\n",
            "1 13\n",
            "0 18\n",
            "2 18\n",
            "1 15\n",
            "0 14\n",
            "1 14\n",
            "0 14\n",
            "1 14\n",
            "1 14\n",
            "1 20\n",
            "2 20\n",
            "0 16\n",
            "1 18\n",
            "0 15\n",
            "0 15\n",
            "0 20\n",
            "1 14\n",
            "2 19\n",
            "0 19\n",
            "0 16\n",
            "0 8\n",
            "3 21\n",
            "0 13\n",
            "1 15\n",
            "1 27\n",
            "0 17\n",
            "1 11\n",
            "0 16\n",
            "0 15\n",
            "0 16\n",
            "0 10\n",
            "0 10\n",
            "0 11\n",
            "0 13\n",
            "0 14\n",
            "1 15\n",
            "0 11\n",
            "1 15\n",
            "0 11\n",
            "0 11\n",
            "0 12\n",
            "0 12\n",
            "0 12\n",
            "1 17\n",
            "2 21\n",
            "2 16\n",
            "1 19\n",
            "2 20\n",
            "0 17\n",
            "1 16\n",
            "1 17\n",
            "0 14\n",
            "0 7\n",
            "1 16\n",
            "2 15\n",
            "0 15\n",
            "0 16\n",
            "1 17\n",
            "1 16\n",
            "1 15\n",
            "0 17\n",
            "0 13\n",
            "1 12\n",
            "0 11\n",
            "0 22\n",
            "0 10\n",
            "0 15\n",
            "1 15\n",
            "1 15\n",
            "1 16\n",
            "1 13\n",
            "0 17\n",
            "1 19\n",
            "0 19\n",
            "0 14\n",
            "1 19\n",
            "0 16\n",
            "0 13\n",
            "1 13\n",
            "0 10\n",
            "1 14\n",
            "2 18\n",
            "1 10\n",
            "0 15\n",
            "0 15\n",
            "0 17\n",
            "2 17\n",
            "0 12\n",
            "0 16\n",
            "0 11\n",
            "0 13\n",
            "0 18\n",
            "0 12\n",
            "1 19\n",
            "1 19\n",
            "0 9\n",
            "1 17\n",
            "1 21\n",
            "2 16\n",
            "0 14\n",
            "0 14\n",
            "0 14\n",
            "3 22\n",
            "1 16\n",
            "1 17\n",
            "0 13\n",
            "0 20\n",
            "0 14\n",
            "1 17\n",
            "1 16\n",
            "0 9\n",
            "1 16\n",
            "2 18\n",
            "0 17\n",
            "1 18\n",
            "1 12\n",
            "2 18\n",
            "1 17\n",
            "2 14\n",
            "0 16\n",
            "1 12\n",
            "0 16\n",
            "0 16\n",
            "0 15\n",
            "0 15\n",
            "0 18\n",
            "0 21\n",
            "0 15\n",
            "0 17\n",
            "0 13\n",
            "0 9\n",
            "1 15\n",
            "1 13\n",
            "1 15\n",
            "0 12\n",
            "0 16\n",
            "0 12\n",
            "1 16\n",
            "0 16\n",
            "0 9\n",
            "0 16\n",
            "0 14\n",
            "1 15\n",
            "2 15\n",
            "1 17\n",
            "0 21\n",
            "1 12\n",
            "0 13\n",
            "0 16\n",
            "0 12\n",
            "0 16\n",
            "1 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IaVlYNYRnxV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0125f851-e2a5-4e9f-baef-b67ced84b664"
      },
      "source": [
        "total_epochs, total_penalties = 0, 0\n",
        "episodes = 100\n",
        "for _ in range(episodes):\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    while True:\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "        epochs += 1\n",
        "        if done:\n",
        "            break\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "print(total_penalties/episodes,total_epochs/episodes)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0 12.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsEJV-iFToMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#improved from 716, 2196 to 0,12.97 with RL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2Pou0FgViC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}